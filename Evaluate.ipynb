{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9fec5de-85e4-47aa-a549-e1f7ca4e205a",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c641aa0c-d69e-432a-96b2-7315bab405f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a107962a-61d2-409f-afaf-03b48134ae6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "110a2dc9-4417-4e20-a728-ce48eafb2965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import jsonlines\n",
    "import pickle\n",
    "import openai\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa06d2ec-3a52-4a3e-ab40-e2041658de95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9854b15-1602-42fd-ae63-a4603e709c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, BertModel\n",
    "from libs.faiss_indexer import DenseFlatIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ab3b4f-ba7b-4e0c-91b7-2b68f05b3351",
   "metadata": {},
   "source": [
    "# Download Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a47782a-3ce4-455f-9c11-eb2b6224b68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dhnam/.pyenv/versions/3.8.17/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea75002-a5f4-4df8-b277-326b0154f1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading dpr_biencoder.400: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.19G/3.19G [02:45<00:00, 19.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/dhnam/SR-DPR/checkpoint/HSC_FAQs/dpr_biencoder.400'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "REPO_ID = 'Nma/DPR_HSC_FAQs'\n",
    "FILENAME = 'dpr_biencoder.400'\n",
    "local_dir = \"/home/dhnam/SR-DPR/checkpoint/HSC_FAQs\"\n",
    "hf_hub_download(repo_id=REPO_ID, filename=FILENAME, local_dir=local_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ac1be6-2691-401a-b86f-dc7d145b9d23",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ba355474-aba9-4954-be93-1eef1845723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_path = \"NlpHUST/vibert4news-base-cased\"\n",
    "# model_path = '/home/dhnam/SR-DPR/checkpoint/HSC_FAQs/dpr_biencoder.400' #New train\n",
    "# model_path = '/home/dhnam/SR-DPR/checkpoint/general/dpr_biencoder.1000' #General\n",
    "model_path = '/home/dhnam/SR-DPR/checkpoint/HSC/dpr_biencoder.400' #A loi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f4ba4c6d-a893-4a79-9683-6cf4ed03eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5697a221-a355-49a1-a9db-39d0725c86fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'RobertaTokenizer'. \n",
      "The class this function is called from is 'BertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "## Load Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6ee5d2a7-a38c-4320-bfb0-182c149d9ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at NlpHUST/vibert4news-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(62000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load Context Model\n",
    "saved_state = torch.load(model_path, map_location=lambda s, t: s)\n",
    "saved_state = saved_state['model_dict']\n",
    "ctx_prefix = \"ctx_model.\"\n",
    "\n",
    "ctx_model_state = {\n",
    "    k[len(ctx_prefix):]: v\n",
    "    for k, v in saved_state.items()\n",
    "    if k.startswith(ctx_prefix)\n",
    "}\n",
    "# assert len(ctx_model_state) == 200\n",
    "ctx_model = BertModel.from_pretrained(pretrained_model_path)\n",
    "ctx_model.load_state_dict(ctx_model_state,  strict=False)\n",
    "ctx_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "4acf461f-5e05-4b4e-8319-616239c6eb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at NlpHUST/vibert4news-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(62000, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Load Question Model \n",
    "saved_state = torch.load(model_path, map_location=lambda s, t: s)\n",
    "saved_state = saved_state['model_dict']\n",
    "question_prefix = \"question_model.\"\n",
    "question_model_state = {\n",
    "        k[len(question_prefix):]: v\n",
    "        for k, v in saved_state.items()\n",
    "        if k.startswith(question_prefix)\n",
    "    }\n",
    "# assert len(question_model_state) == 200\n",
    "question_model = BertModel.from_pretrained(pretrained_model_path)\n",
    "question_model.load_state_dict(question_model_state, strict= False)\n",
    "question_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9339bd41-e623-469e-8b3b-f78398cb64ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda and torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    ctx_model.to('cuda')\n",
    "    # question_model.to('cuda')\n",
    "else:\n",
    "    device ='cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c42b2d9-10eb-4a23-bcdc-4338828d1e78",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "e2df9b7e-93dc-4156-b476-99c6345ef050",
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('/home/dhnam/SR-DPR/data/hsc_newdata/eval.json') as reader:\n",
    "    data = [item for item in reader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "294122f5-54dc-4797-a2ee-b812543177c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_questions = []\n",
    "base_contents = []\n",
    "question_base_questions = []\n",
    "question_base_contents = []\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    base_questions.append(item['base_question'])\n",
    "    base_contents.append(item['base_content'])\n",
    "    questions = item['questions']\n",
    "    question_base_question = questions['questionbasequestion']\n",
    "    question_base_content = questions['questionbasecontent']\n",
    "    for question in question_base_question:\n",
    "        question_base_questions.append((idx, question))\n",
    "    for question in question_base_content:\n",
    "        question_base_contents.append((idx, question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "5b15edef-bdb5-4cf9-a497-559a5e060664",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/dhnam/SR-DPR/data/hsc_newdata/Corpus_v2.xlsx\"\n",
    "adding_data = pd.read_excel(file_path, None)\n",
    "sheet = \"FAQs\"\n",
    "adding_data = adding_data.get(sheet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "c51464f2-c4a5-41e6-aef3-4acb16043763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding_dataadding_data = adding_data.fillna(' ')\n",
    "adding_data = adding_data.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "61eba2b8-196c-4a43-a9f6-297055e55e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in adding_data['Answer']:\n",
    "    if i == ' ':\n",
    "        continue\n",
    "    base_contents.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "22e529f0-5498-4def-b556-2da4c9fe8c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_index_path = '/home/dhnam/SR-DPR/indexes/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9882b2e6-c99e-4177-a8c2-874326800730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "170it [00:03, 54.60it/s]\n"
     ]
    }
   ],
   "source": [
    "data_to_be_indexed = []\n",
    "with torch.no_grad():\n",
    "    for idx, content in tqdm(enumerate(base_contents)):\n",
    "        all_emb = []\n",
    "        inputs = tokenizer(content, return_tensors=\"pt\", truncation=True, max_length=512) \n",
    "        outputs = ctx_model(input_ids=inputs.input_ids.to(device), attention_mask=inputs.attention_mask.to(device), return_dict=True)\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state # [bsz, seq_len, hidden_size] -> [1, 20, 768]\n",
    "        pooled_output = sequence_output[:, 0, :]\n",
    "        all_emb.append(pooled_output)\n",
    "        all_emb = torch.cat(all_emb, dim=0)\n",
    "        data = {\"article_id\": idx, \"text\": content}\n",
    "        data_to_be_indexed.append((data,  all_emb[0].cpu().numpy()))\n",
    "\n",
    "indexer = DenseFlatIndexer()\n",
    "indexer.init_index(vector_sz=768) #(vector_sz=embeddings[0].shape[0]\n",
    "indexer.index_data(data_to_be_indexed)\n",
    "index_path = os.path.dirname(corpus_index_path)\n",
    "indexer.serialize(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "25fde72b-7531-4b36-89f5-2371e4ca0205",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TP_top5 = Q_FP_top5 = 0\n",
    "Q_TP_top3 = Q_FP_top3 = 0\n",
    "\n",
    "A_TP_top5 = A_FP_top5 = 0\n",
    "A_TP_top3 = A_FP_top3 = 0\n",
    "\n",
    "TP_top5 = FP_top5 = 0\n",
    "TP_top3 = FP_top3 = 0\n",
    "\n",
    "ori_TP_top5 = ori_FP_top5 = 0\n",
    "ori_TP_top3 = ori_FP_top3 = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "adfcdaa9-3465-492f-a1d7-375b7e6ef4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, question in enumerate(base_questions):\n",
    "    query = question\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "    query_tokens = [tokenizer.cls_token] + query_tokens + [tokenizer.sep_token]\n",
    "    query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "    outputs = question_model(input_ids=torch.tensor([query_ids]), return_dict=True)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "    retrieval_results = indexer.search_knn(query_embedding, top_docs=5)\n",
    "    metas = retrieval_results[0][0]\n",
    "    # scores = retrieval_results[0][1]\n",
    "    # scores = [float(score) for score in scores]\n",
    "    # retrieval_results = [{**meta, \"score\": score} for meta, score in zip(metas, scores)]   \n",
    "    retrieval_id = [x['article_id'] for x in metas]\n",
    "    if idx in retrieval_id:\n",
    "        ori_TP_top5 +=1\n",
    "        TP_top5 += 1\n",
    "        if idx in retrieval_id[:3]:\n",
    "            ori_TP_top3+=1\n",
    "            TP_top3 +=1\n",
    "    else:\n",
    "        ori_FP_top5 +=1\n",
    "        ori_FP_top3 +=1\n",
    "        FP_top5 += 1\n",
    "        FP_top3 +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "024e7a23-32a6-4584-b051-32a8914f66b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:08<00:00, 16.86it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(question_base_questions):\n",
    "    idx = data[0]\n",
    "    question = data[1]\n",
    "    query = question\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "    query_tokens = [tokenizer.cls_token] + query_tokens + [tokenizer.sep_token]\n",
    "    query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "    outputs = question_model(input_ids=torch.tensor([query_ids]), return_dict=True)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "    retrieval_results = indexer.search_knn(query_embedding, top_docs=5)\n",
    "    metas = retrieval_results[0][0]\n",
    "    # scores = retrieval_results[0][1]\n",
    "    # scores = [float(score) for score in scores]\n",
    "    # retrieval_results = [{**meta, \"score\": score} for meta, score in zip(metas, scores)]   \n",
    "    retrieval_id = [x['article_id'] for x in metas]\n",
    "    if idx in retrieval_id:\n",
    "        Q_TP_top5 +=1\n",
    "        TP_top5 += 1\n",
    "        if idx in retrieval_id[:3]:\n",
    "            Q_TP_top3+=1\n",
    "            TP_top3 +=1\n",
    "    else:\n",
    "        Q_FP_top5 +=1\n",
    "        Q_FP_top3 +=1\n",
    "        FP_top5 += 1\n",
    "        FP_top3 +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e020ac8a-c943-4a7f-a4bf-f69df5901246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 150/150 [00:09<00:00, 16.25it/s]\n"
     ]
    }
   ],
   "source": [
    "for data in tqdm(question_base_contents):\n",
    "    idx = data[0]\n",
    "    question = data[1]\n",
    "    query = question\n",
    "    query_tokens = tokenizer.tokenize(query)\n",
    "\n",
    "    query_tokens = [tokenizer.cls_token] + query_tokens + [tokenizer.sep_token]\n",
    "    query_ids = tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "    outputs = question_model(input_ids=torch.tensor([query_ids]), return_dict=True)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "\n",
    "    retrieval_results = indexer.search_knn(query_embedding, top_docs=5)\n",
    "    metas = retrieval_results[0][0]\n",
    "    # scores = retrieval_results[0][1]\n",
    "    # scores = [float(score) for score in scores]\n",
    "    # retrieval_results = [{**meta, \"score\": score} for meta, score in zip(metas, scores)]   \n",
    "    retrieval_id = [x['article_id'] for x in metas]\n",
    "    if idx in retrieval_id:\n",
    "        A_TP_top5 +=1\n",
    "        TP_top5 += 1\n",
    "        if idx in retrieval_id[:3]:\n",
    "            A_TP_top3+=1\n",
    "            TP_top3 +=1\n",
    "    else:\n",
    "        A_FP_top5 +=1\n",
    "        A_FP_top3 +=1\n",
    "        FP_top5 += 1\n",
    "        FP_top3 +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f75a46c8-9c51-4533-8afb-d38611c19cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Top@3 with original Question: 0.9148936170212766\n",
      "Accuracy Top@5 with original Question: 0.92\n",
      "Accuracy Top@3 with Generated Question with Question: 0.8357142857142857\n",
      "Accuracy Top@5 with Generated Question with Question: 0.8466666666666667\n",
      "Accuracy Top@3 with Generated Question with Answer: 0.9166666666666666\n",
      "Accuracy Top@5 with Generated Question with Answer: 0.92\n"
     ]
    }
   ],
   "source": [
    "## NEw train\n",
    "print(f\"Accuracy Top@3 with original Question: {ori_TP_top3/(ori_TP_top3+ori_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with original Question: {ori_TP_top5/(ori_TP_top5+ori_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 with Generated Question with Question: {Q_TP_top3/(Q_TP_top3+Q_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with Generated Question with Question: {Q_TP_top5/(Q_TP_top5+Q_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 with Generated Question with Answer: {A_TP_top3/(A_TP_top3+A_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with Generated Question with Answer: {A_TP_top5/(A_TP_top5+A_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 All: {TP_top3/(TP_top3+FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 All: {TP_top5/(TP_top5+FP_top5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dc962dd1-a824-4f82-a6b6-e4e91a9b2b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Top@3 with original Question: 0.9148936170212766\n",
      "Accuracy Top@5 with original Question: 0.92\n",
      "Accuracy Top@3 with Generated Question with Question: 0.903448275862069\n",
      "Accuracy Top@5 with Generated Question with Question: 0.9066666666666666\n",
      "Accuracy Top@3 with Generated Question with Answer: 0.9328859060402684\n",
      "Accuracy Top@5 with Generated Question with Answer: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# general\n",
    "print(f\"Accuracy Top@3 with original Question: {ori_TP_top3/(ori_TP_top3+ori_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with original Question: {ori_TP_top5/(ori_TP_top5+ori_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 with Generated Question with Question: {Q_TP_top3/(Q_TP_top3+Q_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with Generated Question with Question: {Q_TP_top5/(Q_TP_top5+Q_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 with Generated Question with Answer: {A_TP_top3/(A_TP_top3+A_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with Generated Question with Answer: {A_TP_top5/(A_TP_top5+A_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 All: {TP_top3/(TP_top3+FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 All: {TP_top5/(TP_top5+FP_top5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c73f347f-1a8c-4324-ad1e-2579ff23c70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Top@3 with original Question: 0.9565217391304348\n",
      "Accuracy Top@5 with original Question: 0.96\n",
      "Accuracy Top@3 with Generated Question with Question: 0.8226950354609929\n",
      "Accuracy Top@5 with Generated Question with Question: 0.8333333333333334\n",
      "Accuracy Top@3 with Generated Question with Answer: 0.8321678321678322\n",
      "Accuracy Top@5 with Generated Question with Answer: 0.84\n",
      "Accuracy Top@3 All: 0.8454545454545455\n",
      "Accuracy Top@5 All: 0.8542857142857143\n"
     ]
    }
   ],
   "source": [
    "# A Loi\n",
    "print(f\"Accuracy Top@3 with original Question: {ori_TP_top3/(ori_TP_top3+ori_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with original Question: {ori_TP_top5/(ori_TP_top5+ori_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 with Generated Question with Question: {Q_TP_top3/(Q_TP_top3+Q_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with Generated Question with Question: {Q_TP_top5/(Q_TP_top5+Q_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 with Generated Question with Answer: {A_TP_top3/(A_TP_top3+A_FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 with Generated Question with Answer: {A_TP_top5/(A_TP_top5+A_FP_top5)}\")\n",
    "\n",
    "print(f\"Accuracy Top@3 All: {TP_top3/(TP_top3+FP_top3)}\")\n",
    "print(f\"Accuracy Top@5 All: {TP_top5/(TP_top5+FP_top5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb2039f-3f56-412b-978d-259aabb9b908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
